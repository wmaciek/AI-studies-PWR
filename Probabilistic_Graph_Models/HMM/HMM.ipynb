{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Maciej Wilhelmi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Collection, Dict, List, Optional, Tuple\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ukryte Modele Markowa (*Hidden Markov Models*) - wprowadzenie\n",
    "\n",
    "W odróżnieniu od dotychczas rozważanych problemów, przy danych sekwencyjnych mówimy o zbiorach obserwacji, których kolejność odgrywa istotną rolę. Przykładem takich danych są np. serie czasowe, czy też język naturalny.\n",
    "\n",
    "W najprostszym podejściu moglibyśmy zignorować sekwencyjność danych (założyć, że są one *IID*) i potraktować jednym z dotychczasowych modeli. Przez zignorowanie zależności występujących pomiędzy kolejnymi krokami tracimy jednak istotne informacje, przez co możemy nie osiągnąć dobrych rezultatów.\n",
    "\n",
    "## Model Markowa\n",
    "\n",
    "Posłużymy się tutaj przykładem pogody. Załóżmy, że pogoda w danym dniu może być opisana zmienną losową, przyjmującą 3 wartości: $Z = \\{słońce, deszcz, śnieg\\}$. Zakładając, że dane są IID, jedyna informacja, na podstawie której możemy wnioskować z danych to względna częstość dni, gdy wystąpiły poszczególne zjawiska pogodowe. Pogoda jednak mocno zależy od trendów, które mogą trwać wiele dni.\n",
    "\n",
    "Modelem pozwalającym wyrazić takie zależności jest Model Markowa (*Markov Model*). Opisuje on prawdopodobieństwo przejścia pomiędzy poszczególnymi stanami $Z = \\{{z_0=słońce}, {z_1=deszcz}, {z_2=śnieg}\\}$. Przykładowe prawdopodobieństwa przejść pomiędzy stanami zawarte zostały w tabelce:\n",
    "\n",
    "| $$\\Pr(z^{(t)} | z^{(t-1)})$$ | $$z^{(t-1)} = słońce$$ | $$z^{(t-1)} = deszcz$$ | $$z^{(t-1)} = śnieg$$  |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $$z^{(t)} = słońce$$ | 0.75 | 0.18 | 0.07 |\n",
    "| $$z^{(t)} = deszcz$$ | 0.45 | 0.52 | 0.03 |\n",
    "| $$z^{(t)} = śnieg$$  | 0.77 | 0.04 | 0.19 |\n",
    "\n",
    "![](assets/weather-obs.jpg)\n",
    "\n",
    "W tak zdefiniowanym modelu łączne prawdopodobieństwo sekwencji obserwacji dane jest iloczynem:\n",
    "\n",
    "$$P(z^{(1)}, \\dots, z^{(T)}) = \\prod_{t=1}^T P(z^{(t)} | z^{(1)}, \\dots, z^{(t-1)}). \\tag{1}$$\n",
    "\n",
    "Zakładając, że rozkłady warunkowe są niezależne od wszystkich obserwacji z wyjątkiem ostatniej uzyskujemy proces Markowa pierwszego rzędu (*first-order Markov chain*). Wówczas wzór upraszcza się do postaci:\n",
    "\n",
    "$$P(z^{(1)}, \\dots, z^{(T)}) = P(z^{(1)}) \\prod_{t=2}^T P(z^{(t)} | z^{(t-1)}). \\tag{2}$$\n",
    "\n",
    "Łańcuchy wyższego rzędu powstają przez uwzględnianie większej liczby poprzednich obserwacji.\n",
    "\n",
    "Do obliczenia prawdopodobieństwa sekwencji obserwacji dla łańcucha pierwszego rzędu, oprócz macierzy przejść pomiędzy stanami (tabelka - $a$) potrzebować będziemy **rozkładu stanów początkowych** $\\pi$, czyli prawdopodobieństw dla każdego z możliwych stanów na początku łańcucha.\n",
    "\n",
    "## Ukryty Model Markowa\n",
    "\n",
    "Rozwinięcie modeli Markowa, gdy zakładamy, że w modelu występują stany, których nie możemy obserwować (**stany ukryte** - *latent variables* $z$) oraz **obserwacje** (*observations*, czasem *evidence* $x$), które uzależnione są pewien sposób od tych stanów ukrytych. Rozszerzając wcześniejszy model, załóżmy że wcześniej wymienione stany są stanami ukrytymi; obserwować będziemy natomiast stroje kobiet: $X = \\{{x_0=sukienka}, {x_1=płaszcz}, {x_2=parasol}\\}$. Wtedy istotne będą, oprócz prawdopodobieństw przejścia, także prawdopodobieństwa **emisji**, czyli prawdopodobieństwa, że w danym stanie ukrytym zaobserwujemy daną obserwację:\n",
    "\n",
    "$$b_{jk} = P(X=x_k | Z = z_j). \\tag{3}$$\n",
    "\n",
    "Wówczas rozszerzony model będzie wyglądał następująco:\n",
    "\n",
    "![](assets/weather-hid.jpg)\n",
    "\n",
    "Poniższa tabelka zawiera prawdopodobieństwa emisji:\n",
    "\n",
    "| $$\\Pr(x_k | z_j)$$ | $$z_k = słońce$$ | $$z_k = deszcz$$ | $$z_k = śnieg$$  |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $$x_j = sukienka$$ | 0.79 | 0.08 | 0.00 |\n",
    "| $$x_j = płaszcz$$  | 0.13 | 0.25 | 0.92 |\n",
    "| $$x_j = parasol$$  | 0.08 | 0.67 | 0.08 |\n",
    "\n",
    "Ukryte Modele Markowa pozwalają nam na obliczanie łącznego prawdopodobieństwa sekwencji stanów ukrytych przy zadanej sekwencji stanów obserwowanych. Znając łączne prawdopodobieństwo sekwencji stanów ukrytych możemy określić ich najbardziej prawdopodobną sekwencję - znając sekwencję strojów kobiet w kilku kolejnych dniach oraz wspomniane wcześniej prawdopodobieństwa przejścia pomiędzy stanami ukrytymi, prawdopodobieństwa początkowe oraz prawdopodobieństwa emisji, będziemy mogli wnioskować na temat tego, jaka była pogoda w tych dniach.\n",
    "\n",
    "*Źródła: C. M. Bishop: Pattern Recognition and Machine Learning (2006)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicja formalna\n",
    "\n",
    "**Ukryty model Markowa** jest zdefiniowany jako pięcioelementowa krotka $(X, Z, \\pi, a, b)$, gdzie:\n",
    "- $X$ - zbiór obserwacji\n",
    "- $Z$ - zbiór stanów ukrytych\n",
    "- $\\pi$ - rozkład prawdopodobieństwa stanów początkowych; $\\pi_i = P(z^{(1)} = z_i)$ ($\\pi_i$ oznacza prawdopodobieństwo  tego, że sekwencja stanów ukrytych rozpocznie się od stanu $z_i$)\n",
    "- $a$ - macierz prawdopodobieństw przejść między stanami ukrytymi $a_{ij} = P(z^{(t)} = z_j | z^{(t-1)} = z_i)$ ($a_{ij}$ oznacza prawdopodobieństwo przejścia ze stanu $z_i$ do stanu $z_j$)\n",
    "- $b$ - macierz prawdpodobieństw emisji $b_{jk} = P(x^{(t)} = x_k | z^{(t)} = z_j)$ ($b_{jk}$ oznacza prawdopodobieństwo obserwacji symbolu $x_k$ gdy model jest w stanie ukrytym $z_j$)\n",
    "\n",
    "Indeks górny $t$ oznacza pozycję w sekwencji (ang. *timestamp*, *time step*).\n",
    "\n",
    "Poniżej przykład dla kolejności aminokwasów w łańcuchu DNA:\n",
    "\n",
    "![](assets/gene-hmm.png)\n",
    "\n",
    "W tym przykładzie ukryty model Markova ma następujące parametry:\n",
    "$$X = \\{A, C, G, T\\}$$\n",
    "\n",
    "$$Z = \\{H, L\\}$$\n",
    "\n",
    "$$\\pi = [0.5, 0.5]$$\n",
    "\n",
    "$$a = \\begin{pmatrix} 0.5 & 0.5 \\\\ 0.4 & 0.6 \\end{pmatrix}$$\n",
    "\n",
    "$$b = \\begin{pmatrix} 0.2 & 0.3 \\\\ 0.3 & 0.2 \\\\ 0.3 & 0.2 \\\\ 0.2 & 0.3\\end{pmatrix}$$\n",
    "\n",
    "\n",
    "*Źródło: https://www.cis.upenn.edu/~cis262/notes/Example-Viterbi-DNA.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższa klasa zawiera przykładową implementację ukrytego modelu Markova - odpowiednik pięcioelementowej krotki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(\n",
    "        self, \n",
    "        Z: Collection[str], \n",
    "        X: Collection[str], \n",
    "        init_dist: Optional[Dict[str, float]] = None, \n",
    "        transition_probs: Optional[Dict[Tuple[str, str], float]] = None, \n",
    "        emission_probs: Optional[Dict[Tuple[str, str], float]] = None,\n",
    "    ):\n",
    "        self.Z = Z  # Hidden states space\n",
    "        self.X = X  # Observations space\n",
    "\n",
    "        self.pi = init_dist  # P(z_0); z_0 -> p\n",
    "        self.tr = transition_probs  # P(z_t | z_{t-1}); (z_t_1, z_t) -> p\n",
    "        self.em = emission_probs  # P (x_t | z_t); (x_t, z_t) -> p\n",
    "        \n",
    "    @property\n",
    "    def theta(self):\n",
    "        return self.pi, self.tr, self.em\n",
    "    \n",
    "    @theta.setter\n",
    "    def theta(self, values: tuple):\n",
    "        self.pi, self.tr, self.em = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Używając tej klasy możemy zdefiniować taki sam ukryty model Markova jak przedstawiono na powyższym rysunku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1dbf22f40a22c3e6713af662c3ede576",
     "grade": false,
     "grade_id": "default-hmms",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_gene_hmm():\n",
    "    _hmm = HMM(\n",
    "        Z=('H', 'L'),\n",
    "        X=('A', 'C', 'G', 'T'),\n",
    "        init_dist={'H': 0.5, 'L': 0.5},\n",
    "        transition_probs={\n",
    "            ('H', 'H'): 0.5, ('H', 'L'): 0.5,\n",
    "            ('L', 'L'): 0.6, ('L', 'H'): 0.4, \n",
    "        },\n",
    "        emission_probs={\n",
    "            ('A', 'H'): 0.2, ('C', 'H'): 0.3, ('G', 'H'): 0.3, ('T', 'H'): 0.2,\n",
    "            ('A', 'L'): 0.3, ('C', 'L'): 0.2, ('G', 'L'): 0.2, ('T', 'L'): 0.3,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return _hmm\n",
    "\n",
    "def get_weather_hmm():\n",
    "    _hmm = HMM(\n",
    "        Z=('Rainy', 'Sunny'),\n",
    "        X=('Walk', 'Shop', 'Clean'),\n",
    "        init_dist={'Rainy': 0.6, 'Sunny': 0.4},\n",
    "        transition_probs={\n",
    "            ('Rainy', 'Rainy'): 0.7, ('Rainy', 'Sunny'): 0.3,\n",
    "            ('Sunny', 'Rainy'): 0.4, ('Sunny', 'Sunny'): 0.6,\n",
    "        },\n",
    "        emission_probs={\n",
    "            ('Walk', 'Rainy'): 0.1, ('Shop', 'Rainy'): 0.4, ('Clean', 'Rainy'): 0.5,\n",
    "            ('Walk', 'Sunny'): 0.6, ('Shop', 'Sunny'): 0.3, ('Clean', 'Sunny'): 0.1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return _hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wypiszmy parametry modelu w bardziej przystępnej formie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_basic_info(hmm):\n",
    "    \"\"\"Displays parameters of given HMM.\"\"\"\n",
    "    # Initial distribution `pi`\n",
    "    pi_df = pd.DataFrame(\n",
    "        columns=[f'$$z^{{(0)}} = {z_0}$$' for z_0 in hmm.pi.keys()],\n",
    "        index=[\"$$P(z^{{(0)}})$$\"],\n",
    "    )\n",
    "\n",
    "    for z_0, p_z_0 in hmm.pi.items():\n",
    "        pi_df[f'$$z^{{(0)}} = {z_0}$$']['$$P(z^{{(0)}})$$'] = p_z_0\n",
    "        \n",
    "    display(pi_df)\n",
    "    \n",
    "    # Transition matrix `a`\n",
    "    a_df = pd.DataFrame(\n",
    "        columns=[f'$$z^{{(t)}} = {dst}$$' for dst in hmm.Z],\n",
    "        index=[f'$$z^{{(t-1)}} = {src}$$' for src in hmm.Z],\n",
    "    )\n",
    "\n",
    "    for (src, dst), val in hmm.tr.items():\n",
    "        a_df[f'$$z^{{(t)}} = {dst}$$'][f'$$z^{{(t-1)}} = {src}$$'] = val\n",
    "        \n",
    "    display(a_df)\n",
    "    \n",
    "    # Emission matrix `b`\n",
    "    b_df = pd.DataFrame(\n",
    "        columns=[f'$$z^{{(t)}} = {z}$$' for z in hmm.Z],\n",
    "        index=[f'$$x^{{(t)}} = {x}$$' for x in hmm.X],\n",
    "    )\n",
    "\n",
    "    for (x, z), val in hmm.em.items():\n",
    "        b_df[f'$$z^{{(t)}} = {z}$$'][f'$$x^{{(t)}} = {x}$$'] = val\n",
    "    \n",
    "    display(b_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$$z^{(0)} = H$$</th>\n",
       "      <th>$$z^{(0)} = L$$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$$P(z^{{(0)}})$$</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 $$z^{(0)} = H$$ $$z^{(0)} = L$$\n",
       "$$P(z^{{(0)}})$$             0.5             0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$$z^{(t)} = H$$</th>\n",
       "      <th>$$z^{(t)} = L$$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$$z^{(t-1)} = H$$</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$$z^{(t-1)} = L$$</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  $$z^{(t)} = H$$ $$z^{(t)} = L$$\n",
       "$$z^{(t-1)} = H$$             0.5             0.5\n",
       "$$z^{(t-1)} = L$$             0.4             0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$$z^{(t)} = H$$</th>\n",
       "      <th>$$z^{(t)} = L$$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$$x^{(t)} = A$$</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$$x^{(t)} = C$$</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$$x^{(t)} = G$$</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$$x^{(t)} = T$$</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                $$z^{(t)} = H$$ $$z^{(t)} = L$$\n",
       "$$x^{(t)} = A$$             0.2             0.3\n",
       "$$x^{(t)} = C$$             0.3             0.2\n",
       "$$x^{(t)} = G$$             0.3             0.2\n",
       "$$x^{(t)} = T$$             0.2             0.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HERE: zmień jeśli chcesz przeanalizować inne dane\n",
    "hmm = get_gene_hmm()\n",
    "# hmm = get_weather_hmm()\n",
    "\n",
    "hmm_basic_info(hmm=hmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prawdopodobieństwo obserwacji podanej sekwencji\n",
    "W celu obliczenia prawdopodobieństwa $\\mathbb{P}(x^{(1)}, \\ldots, x^{(T)} |\\theta)$ obserwacji danej sekwencji $x^{(1)}, \\ldots, x^{(T)}$ dla modelu określonego za pomocą parametrów $\\theta$, używa się tzw. algorytmu **Forward**. Wyniki obliczeń z kolejnych iteracji algorytmu $\\alpha$ są często zapisywane w postaci tabelki:\n",
    "\n",
    "$$\\alpha = \\forall_{z_j \\in Z} \\forall_{t \\in \\{1..T\\}} \\alpha_j(t)$$\n",
    "\n",
    "Wiersze w tej tabeli odpowiadają stanom ukrytym modelu (tzn. mamy $|Z|$ wierszy), a kolumny - kolejnym obserwowanym symbolom (tzn. mamy $T$ kolumn, gdzie $T$ to długość sekwencji). Poniższy rysunek przestawia obliczenia dla poprzednio pokazanego przykładowego ukrytego modelu Markova (stany ukryte $Z = \\{H, L\\}$) oraz sekwencji $x^{(1)} = G, x^{(2)} = G, x^{(3)} = C, x^{(4)} =A$ ($T = 4$): \n",
    "\n",
    "\n",
    "![](assets/forward.png)\n",
    "\n",
    "Na początku rozważamy pierwszy obserwowany symbol $x_1 = \"G\"$. Dla każdego możliwego początkowego stanu ukrytego $z_1$ obliczamy wartości $\\alpha(1)$ jako iloczyny prawd. rozpoczęcia sekwencji stanów ukrytych od danego stanu (rozkład $\\pi$) oraz prawd. emisji symbolu $x_1$ w tym stanie (macierz emisji $b$). Otrzymujemy zatem $|Z|$ wartości, które tworzą pierwszą kolumnę w tabelce. \n",
    "\n",
    "W kolejnych krokach algorytmu dla danej obserwacji $x^{(t)}$, będziemy zakładać, że w poprzednim kroku $t-1$ model mógł się znajdować w dowolnym stanie ukrytym $z^{(t-1)}$. Musimy zatem obliczyć iloczyn prawd. znajdywania się w danym stanie ukrytym, przejścia do obecnego stanu oraz emisji symbolu w danym stanie. Obliczone iloczyny z każdego poprzedniego stanu ukrytego dodajemy, aby otrzymać prawdopodobieństwo dla kroku $t$. \n",
    "\n",
    "$$\n",
    " \\alpha_j(t) = \n",
    "  \\begin{cases} \n",
    "   \\pi_j b_{jk} & \\text{if } t = 1 \\\\\n",
    "   b_{jk} \\sum_{z_i \\in Z} \\alpha_i(t - 1) a_{ij} & \\text{if } t > 1\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Zakładamy, że w danym momencie $t$ obserwujemy symbol $x_k$ (stąd oznaczenie $b_{jk}$).\n",
    "\n",
    "Ostatecznie prawdpodobieństwo obserwacji danej sekwencji $x^{(1)}, \\ldots, x^{(T)}$ obliczamy jako sumę wartości w ostatniej kolumnie tabelki $\\alpha$, tzn.\n",
    "\n",
    "$$\\mathbb{P}(x^{(1)}, \\ldots, x^{(T)}|\\theta) = \\sum_{z_i \\in Z} \\alpha_i(T)$$\n",
    "\n",
    "\n",
    "*Source: https://www.cis.upenn.edu/~cis262/notes/Example-Viterbi-DNA.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1 (1.5 + 0.5 pkt)\n",
    "**a)** Zaimplementuj, zgodnie z powyższym opisem, algorytm **Forward** - funkcja `forward()`. W celu uniknięcia problemów ze stabilnością numeryczną obliczaj logarytmy prawdopodobieństw. Wykorzystaj podaną funkcję `logsumexp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    \"\"\"Compute sum of log-probs using log-sum-exp trick.\"\"\"\n",
    "    x = np.array(x)\n",
    "    x_max = max(x)\n",
    "    return np.log(np.sum(np.exp(x - x_max))) + x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.Z = Z  # Hidden states space\n",
    "# self.X = X  # Observations space\n",
    "\n",
    "# self.pi = init_dist  # P(z_0); z_0 -> p\n",
    "# self.tr = transition_probs  # P(z_t | z_{t-1}); (z_t_1, z_t) -> p\n",
    "# self.em = emission_probs  # P (x_t | z_t); (x_t, z_t) -> p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efa4a5ae689776d16dfc432091c3f63f",
     "grade": false,
     "grade_id": "forward",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward(hmm: HMM, X: List[str]):\n",
    "    \"\"\"Implement the forward algorithm.\n",
    "\n",
    "    :param hmm: Hidden Markov Model object.\n",
    "    :param X: Sequence of elements of length T.\n",
    "    :return: alpha. Dict of log-probabilities, where each key (t, z) denotes\n",
    "        that any particular state z is chosen at the step t.\n",
    "    \"\"\"\n",
    "    alpha = {}  # (t, z_j) -> log_p\n",
    "    \n",
    "    for t,x in enumerate(X):  # pętla po obserwacjach (np.: 0-3)\n",
    "        for z in hmm.Z:  # pętla po stanach ukrytych\n",
    "            if t == 0:  # dla stanu początkowego\n",
    "                alpha[(t,z)] = np.log(hmm.pi[z]) + np.log(hmm.em[(X[t],z)])\n",
    "            else:\n",
    "                alpha[(t,z)] = logsumexp(\n",
    "                    [alpha[(t-1, zz)] + np.log(hmm.tr[(zz,z)]) + np.log(hmm.em[(X[t],z)]) \n",
    "                     for zz in hmm.Z]\n",
    "                )    \n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'H'): -1.8971199848858813,\n",
       " (0, 'L'): -2.3025850929940455,\n",
       " (1, 'H'): -3.366795954944823,\n",
       " (1, 'L'): -3.611918412977808,\n",
       " (2, 'H'): -4.777739451339367,\n",
       " (2, 'L'): -5.007141404842,\n",
       " (3, 'H'): -6.588065947895377,\n",
       " (3, 'L'): -6.004975409135929}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "forward(hmm, ['G', 'G', 'C', 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11cd581d30aabda4c792fff0de94d108",
     "grade": true,
     "grade_id": "forward-tests",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testy ukryte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Następnie zaimplementuj funkcję `score_observation_sequence`, która oblicza prawdopodobieństwo obserwacji danej sekwencji $X = x^{(1)}, \\ldots, x^{(T)}$ pod warunkiem parametrów modelu $\\theta$. Tutaj również oblicz logarytm prawdpodobieństwa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b08b00b53b97d1f3d05a57b2f30717b2",
     "grade": false,
     "grade_id": "score-observation-sequence",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def score_observation_sequence(hmm: HMM, X: List[str]):\n",
    "    \"\"\"Computes the probability of observing a given sequence.\n",
    "\n",
    "    :param hmm: Hidden Markov Model object.\n",
    "    :param X: Sequence of elements of length T.\n",
    "    :return: Tuple of log probability and alpha matrix calculated using\n",
    "        `forward` step. Log probabilities are estimated as a sum of log\n",
    "        probs over each possible state in the last step of alpha. Note, that\n",
    "        these probabilities does not have to sum to one, since they\n",
    "        describe probability of all paths leading to a particular state.\n",
    "    \"\"\"\n",
    "    if any(param is None for param in hmm.theta):\n",
    "        raise RuntimeError('Model must be trained first!')\n",
    "    \n",
    "    alpha = forward(hmm=hmm, X=X)\n",
    "    log_p = None\n",
    "    \n",
    "    # TU WPISZ KOD\n",
    "    fin_probs = []\n",
    "    \n",
    "    for key, value in alpha.items():\n",
    "        if key[0] == len(X)-1:  # dla ostatnich wartości\n",
    "            fin_probs.append(alpha[key])\n",
    "    \n",
    "    log_p = logsumexp(fin_probs)\n",
    "    \n",
    "    return log_p, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.561462936154913,\n",
       " {(0, 'H'): -1.8971199848858813,\n",
       "  (0, 'L'): -2.3025850929940455,\n",
       "  (1, 'H'): -3.366795954944823,\n",
       "  (1, 'L'): -3.611918412977808,\n",
       "  (2, 'H'): -4.777739451339367,\n",
       "  (2, 'L'): -5.007141404842,\n",
       "  (3, 'H'): -6.588065947895377,\n",
       "  (3, 'L'): -6.004975409135929})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "score_observation_sequence(hmm, ['G', 'G', 'C', 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2fe29fe47640ef82ee66cc75c6aac9d",
     "grade": true,
     "grade_id": "score-observation-sequence-tests",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testy ukryte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(scores, T, Z, name):\n",
    "    df = pd.DataFrame(\n",
    "        columns=[f'$$t = {t}$$' for t in range(T)],\n",
    "        index=[f'$$z = {z}$$' for z in Z],\n",
    "    )\n",
    "\n",
    "    for (t, z), v in scores.items():\n",
    "        df[f'$$t = {t}$$'][f'$$z = {z}$$'] = np.round(v, 4)\n",
    "\n",
    "    display(Markdown(f'### {name} table'))\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Probability of observing X: -12.48287649154717 (actual probability: 3.7910160355200114e-06)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Forward table"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$$t = 0$$</th>\n",
       "      <th>$$t = 1$$</th>\n",
       "      <th>$$t = 2$$</th>\n",
       "      <th>$$t = 3$$</th>\n",
       "      <th>$$t = 4$$</th>\n",
       "      <th>$$t = 5$$</th>\n",
       "      <th>$$t = 6$$</th>\n",
       "      <th>$$t = 7$$</th>\n",
       "      <th>$$t = 8$$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$$z = H$$</th>\n",
       "      <td>-1.8971</td>\n",
       "      <td>-3.3668</td>\n",
       "      <td>-4.7777</td>\n",
       "      <td>-6.5881</td>\n",
       "      <td>-7.596</td>\n",
       "      <td>-9.3736</td>\n",
       "      <td>-10.3766</td>\n",
       "      <td>-12.1539</td>\n",
       "      <td>-13.5624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$$z = L$$</th>\n",
       "      <td>-2.3026</td>\n",
       "      <td>-3.6119</td>\n",
       "      <td>-5.0071</td>\n",
       "      <td>-6.005</td>\n",
       "      <td>-7.7433</td>\n",
       "      <td>-8.7823</td>\n",
       "      <td>-10.5232</td>\n",
       "      <td>-11.5626</td>\n",
       "      <td>-12.898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          $$t = 0$$ $$t = 1$$ $$t = 2$$ $$t = 3$$ $$t = 4$$ $$t = 5$$  \\\n",
       "$$z = H$$   -1.8971   -3.3668   -4.7777   -6.5881    -7.596   -9.3736   \n",
       "$$z = L$$   -2.3026   -3.6119   -5.0071    -6.005   -7.7433   -8.7823   \n",
       "\n",
       "          $$t = 6$$ $$t = 7$$ $$t = 8$$  \n",
       "$$z = H$$  -10.3766  -12.1539  -13.5624  \n",
       "$$z = L$$  -10.5232  -11.5626   -12.898  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HERE: zmień jeśli chcesz przeanalizować inne dane\n",
    "sample_X = ['G', 'G', 'C', 'A','C', 'T', 'G', 'A', 'A']\n",
    "# sample_X = ['Walk', 'Clean', 'Clean', 'Clean', 'Shop']\n",
    "\n",
    "log_p, alpha = score_observation_sequence(hmm=hmm, X=sample_X)\n",
    "print(f'Log-Probability of observing X: {log_p} (actual probability: {np.exp(log_p)})')\n",
    "\n",
    "print_table(scores=alpha, T=len(sample_X), Z=hmm.Z, name='Forward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dekodowanie Viterbiego\n",
    "Kolejnym zadaniem związanym z ukrytym modelem Markova jest znajdywanie najbardziej prawdopodobnej sekwencji stanów ukrytych $Z_{max}$ dla podanej sekwencji obserwacji $X$. \n",
    "\n",
    "$$Z_{max} = \\max_{z^{(1)}, \\ldots, z^{(T)}} \\mathbb{P}(z^{(1)},z^{(2)},\\ldots,z^{(T)}, x^{(1)},x^{(2)}, \\ldots, x^{(T)} | \\theta)$$\n",
    "\n",
    "Stosowany jest tutaj algorytm **dekodowania Viterbiego** (*Viterbi decoding*). Działa on na podobnej zasadzie jak algorytm **Forward**, przy czym zamiast obliczać sumę po wszystkich poprzednich stanach ukrytych, tutaj wybieramy stan który miał największe prawdopodobieństwo. \n",
    "\n",
    "Będziemy obliczać wartości $\\omega$. Ponownie, przy oznaczeniach zakładamy, że w danym momencie $t$ obserwujemy symbol $x_k$ (stąd oznaczenie $b_{jk}$).\n",
    "\n",
    "$$\\omega_i(t) = \\max_{z^{(1)}, \\ldots, z^{(t)}} \\mathbb{P}(z^{(1)}, \\ldots, z^{(t-1)}, \\mathbf{z^{(t)} = z_i}, x^{(1)}, \\ldots, x^{(t-1)}, x^{(t)} | \\theta)$$\n",
    "\n",
    "$$\n",
    " \\omega_j(t) = \n",
    "  \\begin{cases} \n",
    "   \\pi_j b_{jk} & \\text{if } t = 1 \\\\\n",
    "   b_{jk} \\max_{z_i \\in Z}  a_{ij} \\omega_i(t - 1) & \\text{if } t > 1\n",
    "  \\end{cases}\n",
    "$$ \n",
    "\n",
    "W trakcie obliczania tych wartości, musimy zapamiętywać, który ze stanów wybraliśmy. Dzięki temu będziemy w stanie łatwo odtworzyć najbardziej prawdopodobną ścieżką stanów ukrytych (*backtracking*).\n",
    "\n",
    "![](assets/viterbi-decoding.png)\n",
    "\n",
    "*Source: https://www.cis.upenn.edu/~cis262/notes/Example-Viterbi-DNA.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2 (1.5 pkt.)\n",
    "Zaimplementuj funkcję `viterbi_decode`, która wyznaczy najbardziej prawdopodobną ścieżkę stanów ukrytych dla podanego modelu (parametry $\\theta$) oraz obserwacji $X$. Pamiętaj o obliczaniu logarytmów prawdopodobieństw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6fa27198e1f00c1d2e44fbf3da21c5f",
     "grade": false,
     "grade_id": "viterbi-decoding",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def viterbi_decode(hmm: HMM, X: List[str]):\n",
    "    \"\"\"Implement the Viterbi decoding algorithm.\n",
    "\n",
    "    :param hmm: Hidden Markov Model object.\n",
    "    :param X: Sequence of elements of length T.\n",
    "    :return: Tuple of: the most probable path of hidden states and the\n",
    "        omega matrix. The most probability is chosen in a greedy fashion\n",
    "        by takin the most probable hidden state in each step. Omega describes\n",
    "        hidden states probabilities at each `t` step.\n",
    "    \"\"\"\n",
    "    if any(param is None for param in hmm.theta):\n",
    "        raise RuntimeError('Model must be trained first!')\n",
    "    \n",
    "    # Compute scores (forward pass)\n",
    "    omega = {}  # (t, z_t) -> max (log_p, z_{t-1}) ?\n",
    "    max_Z = []\n",
    "    path_id = []\n",
    "    \n",
    "    for t,x in enumerate(X):  # pętla po obserwacjach (np.: 0-3)\n",
    "        for z in hmm.Z:  # pętla po stanach ukrytych\n",
    "            if t == 0:  # dla stanu początkowego\n",
    "                omega[(t,z)] = [(np.log(hmm.pi[z]) + np.log(hmm.em[(X[t],z)])), 0]  # 0 jako war początkowy\n",
    "            else:\n",
    "                omega[(t,z)] = [(np.log(hmm.em[(X[t],z)]) + max([omega[(t-1,zz)][0] + np.log(hmm.tr[(zz,z)]) \n",
    "                                                                 for zz in hmm.Z])),\n",
    "                                path_id\n",
    "                               ]\n",
    "                \n",
    "        m_pom = max([omega[(t,zz)][0] for zz in hmm.Z])\n",
    "        path_id = ([k[1] for k, v in omega.items() if v[0] == m_pom][0])\n",
    "        max_Z.append(path_id)\n",
    "    # TU WPISZ KOD\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    return max_Z, omega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L'],\n",
       " {(0, 'H'): [-1.8971199848858813, 0],\n",
       "  (0, 'L'): [-2.3025850929940455, 0],\n",
       "  (1, 'H'): [-3.7942399697717626, 'H'],\n",
       "  (1, 'L'): [-4.199705077879927, 'H'],\n",
       "  (2, 'H'): [-5.691359954657644, 'H'],\n",
       "  (2, 'L'): [-6.0968250627658085, 'H'],\n",
       "  (3, 'H'): [-7.99394504765169, 'H'],\n",
       "  (3, 'L'): [-7.588479939543525, 'H'],\n",
       "  (4, 'H'): [-9.708743475743615, 'L'],\n",
       "  (4, 'L'): [-9.708743475743615, 'L'],\n",
       "  (5, 'H'): [-12.01132856873766, 'H'],\n",
       "  (5, 'L'): [-11.423541903835542, 'H'],\n",
       "  (6, 'H'): [-13.543805440035632, 'L'],\n",
       "  (6, 'L'): [-13.543805440035632, 'L'],\n",
       "  (7, 'H'): [-15.846390533029677, 'H'],\n",
       "  (7, 'L'): [-15.258603868127558, 'H'],\n",
       "  (8, 'H'): [-17.784332512435814, 'L'],\n",
       "  (8, 'L'): [-16.973402296219486, 'L']})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_decode(hmm=hmm, X=sample_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "537c2c504410eaa7443d1e7fb221cec1",
     "grade": true,
     "grade_id": "viterbi-decoding-tests",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testy ukryte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_viterbi_table(scores, T, Z):\n",
    "    def highlight_max(s):\n",
    "        is_max = s == s.max()\n",
    "        return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        columns=[f'$$t = {t}$$' for t in range(T)],\n",
    "        index=[f'$$z = {z}$$' for z in Z],\n",
    "    )\n",
    "\n",
    "    for (t, z), v in scores.items():\n",
    "        df[f'$$t = {t}$$'][f'$$z = {z}$$'] = (np.round(v[0], 4), v[1])\n",
    "\n",
    "\n",
    "    df = df.style.apply(highlight_max)\n",
    "\n",
    "    display(Markdown('### Viterbi table'))\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable sequence of hidden states: ['H', 'H', 'H', 'L', 'H', 'L', 'H', 'L', 'L']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Viterbi table"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_2853b_row0_col0,#T_2853b_row0_col1,#T_2853b_row0_col2,#T_2853b_row0_col4,#T_2853b_row0_col6,#T_2853b_row1_col3,#T_2853b_row1_col4,#T_2853b_row1_col5,#T_2853b_row1_col6,#T_2853b_row1_col7,#T_2853b_row1_col8{\n",
       "            background-color:  yellow;\n",
       "        }</style><table id=\"T_2853b_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >$$t = 0$$</th>        <th class=\"col_heading level0 col1\" >$$t = 1$$</th>        <th class=\"col_heading level0 col2\" >$$t = 2$$</th>        <th class=\"col_heading level0 col3\" >$$t = 3$$</th>        <th class=\"col_heading level0 col4\" >$$t = 4$$</th>        <th class=\"col_heading level0 col5\" >$$t = 5$$</th>        <th class=\"col_heading level0 col6\" >$$t = 6$$</th>        <th class=\"col_heading level0 col7\" >$$t = 7$$</th>        <th class=\"col_heading level0 col8\" >$$t = 8$$</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_2853b_level0_row0\" class=\"row_heading level0 row0\" >$$z = H$$</th>\n",
       "                        <td id=\"T_2853b_row0_col0\" class=\"data row0 col0\" >(-1.8971, 0)</td>\n",
       "                        <td id=\"T_2853b_row0_col1\" class=\"data row0 col1\" >(-3.7942, 'H')</td>\n",
       "                        <td id=\"T_2853b_row0_col2\" class=\"data row0 col2\" >(-5.6914, 'H')</td>\n",
       "                        <td id=\"T_2853b_row0_col3\" class=\"data row0 col3\" >(-7.9939, 'H')</td>\n",
       "                        <td id=\"T_2853b_row0_col4\" class=\"data row0 col4\" >(-9.7087, 'L')</td>\n",
       "                        <td id=\"T_2853b_row0_col5\" class=\"data row0 col5\" >(-12.0113, 'H')</td>\n",
       "                        <td id=\"T_2853b_row0_col6\" class=\"data row0 col6\" >(-13.5438, 'L')</td>\n",
       "                        <td id=\"T_2853b_row0_col7\" class=\"data row0 col7\" >(-15.8464, 'H')</td>\n",
       "                        <td id=\"T_2853b_row0_col8\" class=\"data row0 col8\" >(-17.7843, 'L')</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2853b_level0_row1\" class=\"row_heading level0 row1\" >$$z = L$$</th>\n",
       "                        <td id=\"T_2853b_row1_col0\" class=\"data row1 col0\" >(-2.3026, 0)</td>\n",
       "                        <td id=\"T_2853b_row1_col1\" class=\"data row1 col1\" >(-4.1997, 'H')</td>\n",
       "                        <td id=\"T_2853b_row1_col2\" class=\"data row1 col2\" >(-6.0968, 'H')</td>\n",
       "                        <td id=\"T_2853b_row1_col3\" class=\"data row1 col3\" >(-7.5885, 'H')</td>\n",
       "                        <td id=\"T_2853b_row1_col4\" class=\"data row1 col4\" >(-9.7087, 'L')</td>\n",
       "                        <td id=\"T_2853b_row1_col5\" class=\"data row1 col5\" >(-11.4235, 'H')</td>\n",
       "                        <td id=\"T_2853b_row1_col6\" class=\"data row1 col6\" >(-13.5438, 'L')</td>\n",
       "                        <td id=\"T_2853b_row1_col7\" class=\"data row1 col7\" >(-15.2586, 'H')</td>\n",
       "                        <td id=\"T_2853b_row1_col8\" class=\"data row1 col8\" >(-16.9734, 'L')</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f88a99dc310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_Z, scores = viterbi_decode(hmm=hmm, X=sample_X)\n",
    "\n",
    "print('Most probable sequence of hidden states:', max_Z)\n",
    "\n",
    "print_viterbi_table(scores=scores, T=len(sample_X), Z=hmm.Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generowanie danych za pomocą ukrytego modelu Markova\n",
    "Możemy użyć ukryty model Markova, aby wygenerować (wypróbkować) sekwencję obserwacji i stanów ukrytych. Na początku losujemy z rozkładu $\\pi$ ukryty stan początkowy $z^{(1)}$. Następnie, korzystając z macierzy przejść (tranzycji) $a$ losujemy kolejne stany ukryte. W tym samym czasie (lub po wygenerowaniu sekwencji stanów ukrytych), korzystając z macierzy emisji $b$, losujemy (dla każdego stanu ukrytego $z^{(t)}$) obserwację $x^{(t)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(hmm: HMM, N: int):\n",
    "    \"\"\"Generates `N` observations and returns both X and Z.\"\"\"\n",
    "    X, Z = [], []\n",
    "\n",
    "    # Draw initial state\n",
    "    z_0 = np.random.choice(\n",
    "        a=hmm.Z,\n",
    "        p=[hmm.pi[z] for z in hmm.Z],\n",
    "    )\n",
    "    Z.append(z_0)\n",
    "\n",
    "    # Draw next states based on transition matrix\n",
    "    z_t_1 = z_0\n",
    "    for _ in range(N - 1):\n",
    "        z_t = np.random.choice(\n",
    "            a=hmm.Z,\n",
    "            p=[hmm.tr[(z_t_1, z)] for z in hmm.Z],\n",
    "        )\n",
    "\n",
    "        Z.append(z_t)\n",
    "        z_t_1 = z_t\n",
    "\n",
    "    # Draw observations\n",
    "    for z_t in Z:\n",
    "        x_t = np.random.choice(\n",
    "            a=hmm.X,\n",
    "            p=[hmm.em[(x, z_t)] for x in hmm.X],\n",
    "        )\n",
    "        X.append(x_t)\n",
    "\n",
    "    return X, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observations = 10\n",
    "X_generated, Z_generated = generate(hmm=hmm, N=num_observations)\n",
    "\n",
    "print('Z (hidden states):', Z_generated)\n",
    "print('X (observations):', X_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie ukrytego modelu Markova\n",
    "Znając przestrzeń stanów ukrytych $Z$ oraz przestrzeń obserwacji $X$, możemy dla podanego zbioru sekwencji obserwacji wyestymować parametry $(\\pi, a, b)$ ukrytego modelu Markova. Najczęściej stosowanymi algorytmami są: **algorytm (uczenia) Viterbiego** oraz **algorytm Baum-Welcha** (zwany również algorytmem **Forward-Backward**). W ramach kursu zajmiemy się algorytmem Viterbiego.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(model, num_seq, seq_length, train_size):\n",
    "    tr_size = int(train_size * num_seq)\n",
    "    te_size = num_seq - tr_size\n",
    "\n",
    "    ds = {\n",
    "        'train': np.array([generate(hmm=model, N=seq_length)[0] for _ in range(tr_size)]),\n",
    "        'test': np.array([generate(hmm=model, N=seq_length)[0] for _ in range(te_size)]),\n",
    "    }\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(n, rng=None):\n",
    "    p = np.random.uniform(size=n)\n",
    "    p /= sum(p)\n",
    "    return p\n",
    "\n",
    "\n",
    "def get_uninitialized_hmm(Z, X):\n",
    "    # Initial distribution    \n",
    "    pi = {z: v for z, v in zip(Z, pdist(len(Z)))}\n",
    "    \n",
    "    # Transition probs\n",
    "    tr = {\n",
    "        (z_t_1, z_t): p\n",
    "        for z_t_1 in Z\n",
    "        for z_t, p in zip(Z, pdist(len(Z)))\n",
    "    }\n",
    "    \n",
    "    # Emission probs\n",
    "    em = {\n",
    "        (x, z_k): p\n",
    "        for z_k in Z\n",
    "        for x, p in zip(X, pdist(len(X)))\n",
    "    }\n",
    "    \n",
    "    _hmm = HMM(\n",
    "        Z=Z, \n",
    "        X=X, \n",
    "        init_dist=pi, \n",
    "        transition_probs=tr, \n",
    "        emission_probs=em,\n",
    "    )\n",
    "    return _hmm\n",
    "\n",
    "\n",
    "def get_uninitialized_gene_hmm():\n",
    "    return get_uninitialized_hmm(\n",
    "        Z=('H', 'L'),\n",
    "        X=('A', 'C', 'G', 'T'),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_uninitialized_weather_hmm():\n",
    "    return get_uninitialized_hmm(\n",
    "        Z=('Rainy', 'Sunny'),\n",
    "        X=('Walk', 'Shop', 'Clean'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_dataset(model=hmm, num_seq=1_000, seq_length=10, train_size=0.8)\n",
    "\n",
    "print('Train =>', dataset['train'].shape)\n",
    "print('Test =>', dataset['test'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorytm Viterbiego (uczenie)\n",
    "Algorytm ten jest przykładem metody Maximum Likelihood Estimation. Pozwala on na estymację / wyuczenie parametrów ukrytego modelu Markova $\\theta = (\\pi, a, b)$. Algorytm jest zdefiniowany następująco:\n",
    "1. Losowo zainicjalizuj parametry $\\theta_0$ oraz ustaw licznik iteracji $q = 0$\n",
    "2. Dopóki nie jest spełniony warunek zatrzymania wykonuj:\n",
    "    - zainicjalizuj liczniki:\n",
    "        - początkowych stanów ukrytych $n^{(\\pi)}$: $\\forall_{z_i \\in Z} \\;n^{(\\pi)}_i = 0$\n",
    "        - macierzy przejść $n^{(a)}$: $\\forall_{z_i \\in Z} \\; \\forall_{z_j \\in Z} \\; n^{(a)}_{ij} = 0$ \n",
    "        - macierzy emisji $n^{(b)}$: $\\forall_{z_j \\in Z}\\; \\forall_{x_k \\in X}\\; n^{(b)}_{jk} = 0$\n",
    "    - dla każdej sekwencji obserwacji $x \\in X_{train}$:\n",
    "        - znajdź najbardziej prawdopodobną sekwencję stanów ukrytych $Z_{max}$ dla sekwencji obserwacji $x$ używając parametrów $\\theta_q$ (użyj algorytmu dekodowania Viterbiego)\n",
    "        - zaktualizuj liczniki $n^{(\\pi)}$, $n^{(a)}$ oraz $n^{(b)}$ na podstawie $Z_{max}$ oraz $x$, tzn:\n",
    "            - zwiększ o jeden licznik $n^{(\\pi)}_i$ dla pierwszego stanu ukrytego $Z_{max}^{(1)} = z_i$\n",
    "            - dla każdej pary $(Z_{max}^{(t-1)} = z_i, Z_{max}^{(t)} = z_j)$ zwiększ o jeden licznik $n^{(a)}_{ij}$\n",
    "            - dla każdej pary $(Z_{max}^{(t)} = z_j, x^{(t)} = x_k)$ zwiększ o jeden licznik $n^{(b)}_{jk}$\n",
    "    - oblicz nowe parametry $\\theta_{q+1} = (\\pi, a, b)$ normalizując liczniki $n^{(\\pi)}$, $n^{(a)}$, $n^{(b)}$\n",
    "    \n",
    "    $$\\pi_i = \\frac{n^{(\\pi)}_i}{\\sum_{z_j \\in Z} n^{(\\pi)}_j}$$\n",
    "    \n",
    "    $$a_{ij} = \\frac{n^{(a)}_{ij}}{\\sum_{z_k \\in Z} n^{(a)}_{ik}}$$\n",
    "\n",
    "    $$b_{jk} = \\frac{n^{(b)}_{jk}}{\\sum_{x_p \\in X} n^{(b)}_{jp}}$$\n",
    "    - $q = q + 1$\n",
    "3. Zwróć model z parametrami $\\theta_{q_{max}}$\n",
    "\n",
    "\n",
    "Jako kryterium zatrzymania będziemy używać dwóch warunków (zatrzymaj jeśli którykolwiek niespełniony):\n",
    "- maksymalna liczba iteracji (epok);  \n",
    "$$q < max\\_epochs$$\n",
    "- zmiana wartości parametrów jest mniejsza niż zadany próg; \n",
    "$$(\\theta_{q+1} - \\theta_q)^2 > \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3 (1.5 pkt.)\n",
    "Zaimplementuj algorythm Viterbiego (uczenie) zgodnie z powyższym opisem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_loss(prev_theta, theta):\n",
    "    if prev_theta == (None, None, None):\n",
    "        return 1\n",
    "\n",
    "    prev_pi, prev_tr, prev_em = prev_theta\n",
    "    pi, tr, em = theta\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Initial dist loss\n",
    "    for k in prev_pi.keys():\n",
    "        loss += (prev_pi[k] - pi[k]) ** 2\n",
    "\n",
    "    # Transition prob. loss\n",
    "    for k in prev_tr.keys():\n",
    "        loss += (prev_tr[k] - tr[k]) ** 2\n",
    "\n",
    "    # Emission prob. loss\n",
    "    for k in prev_em.keys():\n",
    "        loss += (prev_em[k] - em[k]) ** 2\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hmm(hmm, X):\n",
    "    return logsumexp([\n",
    "        score_observation_sequence(hmm=hmm, X=x)[0]\n",
    "        for x in X\n",
    "    ]) - np.log(len(X))\n",
    "\n",
    "\n",
    "def training_plots(hmm, losses, probas, X_test):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(15, 4))\n",
    "    ax[0].plot(range(len(losses)), losses, marker='x', linestyle='--')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "\n",
    "    ax[1].plot(range(len(probas)), probas, marker='x', linestyle='--', label='Train')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Train log-prob')\n",
    "\n",
    "    logp = test_hmm(hmm=hmm, X=X_test)\n",
    "    print(f'Test log-p: {logp}')\n",
    "    ax[1].axhline(logp, linestyle='--', color='r', label='Test')\n",
    "    ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0f7eeffd7beee976855fe01c0663bee",
     "grade": false,
     "grade_id": "viterbi-learning",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def fit_viterbi(\n",
    "    hmm: HMM, \n",
    "    X_train: List[List[str]], \n",
    "    smoothing: int = 1,\n",
    "    eps: float = 1e-4, \n",
    "    max_epochs: int = 30, \n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"Estimates the model's parameters using Viterbi learning algorithm.\"\"\"\n",
    "    prev_theta = (None, None, None)\n",
    "\n",
    "    logs = {\n",
    "        'losses': [],\n",
    "        'probas': [],\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(\n",
    "        iterable=range(max_epochs), \n",
    "        desc='Epochs',\n",
    "        disable=not verbose,\n",
    "    ):\n",
    "        # Update params\n",
    "        prev_theta = hmm.theta\n",
    "        hmm.theta = estimate_parameters(hmm, X_train, smoothing=smoothing)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = parameter_loss(prev_theta, hmm.theta)\n",
    "        probas = logsumexp([\n",
    "            score_observation_sequence(hmm=hmm, X=x)[0]\n",
    "            for x in X_train\n",
    "        ]) - np.log(len(X_train))\n",
    "\n",
    "        logs['losses'].append(loss)\n",
    "        logs['probas'].append(probas)\n",
    "\n",
    "        # Logging\n",
    "        if verbose:\n",
    "            print(f'Epoch: {epoch} => '\n",
    "                  f'Loss: {np.round(loss, 5)}, '\n",
    "                  f'Log-prob: {probas}')\n",
    "\n",
    "        # Stopping condition\n",
    "        if loss < eps:\n",
    "            break\n",
    "    \n",
    "    return logs\n",
    "\n",
    "\n",
    "\n",
    "def estimate_parameters(hmm: HMM, X_train: List[List[str]], smoothing: int):\n",
    "    \"\"\"Estimate parameters of the HMM usng Viterbi algorithm.\n",
    "\n",
    "    :param hmm: Hidden Markov Model object.\n",
    "    :param X: Sequence of elements of length T.\n",
    "    :return: Tuple of parameters of the HMM: initial state probability pi,\n",
    "        transition probability matrix a, and emission probabiliy matrix b.\n",
    "    \"\"\"\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return pi, tr, em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cf6c3151ea394bb98669a8c026a8888",
     "grade": true,
     "grade_id": "viterbi-learning-tests",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testy ukryte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE: zmień jeśli chcesz przeanalizować inne dane\n",
    "hmm_viterbi = get_uninitialized_gene_hmm()\n",
    "# hmm_viterbi = get_uninitialized_weather_hmm()\n",
    "\n",
    "hmm_basic_info(hmm_viterbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = fit_viterbi(hmm=hmm_viterbi, X_train=dataset['train'])\n",
    "hmm_basic_info(hmm_viterbi)\n",
    "training_plots(hmm_viterbi, logs['losses'], logs['probas'], dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ee18e201a1a9d06826414e805f0152c",
     "grade": false,
     "grade_id": "baum-welch",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Zadanie dodatkowe (1 p.)\n",
    "Drugim algorytmem uczenia HMM jest algorytm Baum-Welcha (Forward-Backward). Zadanie dodatkowe polega na implementacji tego algorytmu w podobny sposób jak algorytm Viterbiego.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dodatkowe źródła**:\n",
    "- [Forward+Viterbi] https://www.cis.upenn.edu/~cis262/notes/Example-Viterbi-DNA.pdf\n",
    "- [Full intro to HMM + Forward + Viterbi] https://www.seas.upenn.edu/~cis262/notes/cis262-hmm.pdf\n",
    "- [Intro to HMM + Forward + Viterbi + Forward-Backward] http://www.cs.tut.fi/kurssit/SGN-24006/PDF/L08-HMMs.pdf\n",
    "- [Baum-Welch] https://en.wikipedia.org/wiki/Baum–Welch_algorithm\n",
    "- [Baum-Welch implementation] http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/\n",
    "- [Forward and backward implementations] http://www.adeveloperdiary.com/data-science/machine-learning/forward-and-backward-algorithm-in-hidden-markov-model/\n",
    "- [Viterbi implementation] http://www.adeveloperdiary.com/data-science/machine-learning/implement-viterbi-algorithm-in-hidden-markov-model-using-python-and-r/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
