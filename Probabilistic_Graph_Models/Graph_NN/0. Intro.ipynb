{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4023363b",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317036ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Maciej Wilhelmi\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8763b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b9ca4",
   "metadata": {},
   "source": [
    "# 0. Wprowadzenie\n",
    "## 0.1. Zakres\n",
    "Podczas ostatnich zajęć zapoznaliśmy się z:\n",
    "- biblioteką PyTorch-Geometric i sposobem zapisu grafów (obiekt `Data`)\n",
    "- typowymi zadaniami realizowanymi w uczeniu reprezentacji grafów (klasyfikacja wierzchołków, predykcja krawędzi oraz klasyfikacja grafów)\n",
    "- transduktywnymi modelami opartymi o błądzenia losowe (DeepWalk, Node2vec).\n",
    "\n",
    "W trakcie obecnego laboratorium będziemy kontynuować pracę z modelami uczenia reprezentacji dla grafów i poznamy modele tzw. **grafowych sieci neuronowych**. Są to przykłady z rodziny modeli induktywnych, tzn. potrafią wyznaczać reprezentacje dla wcześniej nieobserwowanych przykładów (wierzchołków, krawędzi, grafów)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a29e18",
   "metadata": {},
   "source": [
    "## 0.2. Motywacja\n",
    "\n",
    "### Geometric Deep Learning blueprint\n",
    "W ramach Wykładu 3 wprowadziliśmy blueprint geometrycznego uczenia głębokiego. Jednym z podstawowych punktów tego opisu było założenie o lokalnie działających funkcjach ekwiwariantnych. W połączeniu z m.in. operacjami redukcji (*pooling*) można było zbudować model, który potrafi dobrze opisywać złożone zależności w danych.\n",
    "\n",
    "W szczególności zauważyliśmy, że pojęcie lokalności w grafie jest oparte na definicji sąsiedztwa wierzchołków. Sąsiedztwem bezpośrednim $\\mathcal{N}_u$ (1-skokowym; ang. *one hop neighborhood*) wierzchołka $u$ nazywamy:\n",
    "$$\\mathcal{N}_u = \\{v \\in \\mathcal{V}: (u, v) \\in \\mathcal{E} \\lor (v, u) \\in \\mathcal{E} \\}$$\n",
    "\n",
    "Lokalnie działająca funkcja $\\phi(\\mathbf{x}_u, \\mathbf{X}_{\\mathcal{N}_u})$, która wykorzystuje atrybuty danego wierzchołka $\\mathbf{x}_u$ oraz cechy jego sąsiadów $\\mathbf{X}_{\\mathcal{N}_u}$, aby wyznaczyć wektor reprezentacji wierzchołka, pozwala na zbudowanie ekwiwariantnej względem permutacji funkcji $\\mathbf{F}$:\n",
    "\n",
    "$$\n",
    "    \\mathbf{F}(\\mathbf{X}, \\mathbf{A}) = \n",
    "        \\begin{bmatrix}\n",
    "            - \\phi(\\mathbf{x}_1, \\mathbf{X}_{\\mathcal{N}_1}) -\\\\\n",
    "            - \\phi(\\mathbf{x}_2, \\mathbf{X}_{\\mathcal{N}_2}) -\\\\\n",
    "            \\vdots \\\\\n",
    "            - \\phi(\\mathbf{x}_n, \\mathbf{X}_{\\mathcal{N}_n}) -\\\\\n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Połączenie wielu takich funkcji pozwala na opisanie szerszych sąsiedztw w grafie, tzn. połącznie dwóch warstw (funkcji) pozwala opisać sąsiedztwo dwu-skokowe (2-hop), połączenie trzech funkcji – 3-hop, itd. W trakcie obecnego laboratorium zobaczymy jak można zdefiniować funkcję $\\phi$.\n",
    "\n",
    "![](./assets/local_graph_function.png)\n",
    "**Źródło**: M. M. Bronstein, J. Bruna, T. Cohen, P. Veličković, *Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges*\n",
    "\n",
    "\n",
    "### Weisfeiler-Lehman kernel\n",
    "Inną perspektywę na budowanie cech opisujących grafy poznaliśmy na Wykładzie 4, gdzie wprowadziliśmy algorytm kolorowania wierzchołków w celu zbadania izomorfizmu pary grafów. Na podstawie początkowo przypisanych kolorów wierzchołków, iteracyjnie każdy wierzchołek agregował multi-zbiór kolorów jego sąsiadów a następnie za pomocą funkcji haszującej $\\text{HASH}$ obliczany był nowy kolor wierzchołka:\n",
    "\n",
    "$$ c^{(k+1)}(v) = \\text{HASH}(\\{c^{(k)}(v), \\{c^{(k)}(u)\\}_{u \\in \\mathcal{N}(v)}\\})$$\n",
    "\n",
    "Powtórzenie tego kroku agregacji $K$-krotnie pozwalało opisać $K$-skokowe sąsiedztwo wierzchołków.\n",
    "\n",
    "\n",
    "**Uwaga:** Zauważmy, że przeprowadzanie $K$ iteracji jest \"równoważne\" z połączeniem $K$-warstw (funkcji) z poprzedniego przykładu.\n",
    "\n",
    "### Dodatkowe materiały\n",
    "W przypadku chęci lepszego zrozumienia oraz uzupełnienia wiedzy dot. grafowych sieci neuronowych poleca się poniższe źródła:\n",
    "- [(part 1) A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/)\n",
    "- [(part 2) Understanding Convolutions on Graphs](https://distill.pub/2021/understanding-gnns/)\n",
    "- Na dłuższą lekturę: [Graph Representation Learning (William L. Hamilton, 2020)](https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
